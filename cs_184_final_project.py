# -*- coding: utf-8 -*-
"""CS 184 Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ydS__k_oCsUqfne9wqNfhW8ebmUNhx8S
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
from tqdm.auto import trange

"""### Load Data"""

# Import data from json file
def read_file(filename):
  lines = []
  with open(filename) as file:
    for line in file:
      lines.append(json.loads(line) )
  df = pd.json_normalize(lines)
  df["helpful_votes"] = df["helpful"].map(lambda votes: votes[0])
  df["num_votes"] = df["helpful"].map(lambda votes: votes[1])
  df["helpful_ratio"] = df["helpful_votes"] / df["num_votes"]
  df = df.drop(columns=['helpful'])

  df["llm_rating"] = pd.to_numeric(df["llm_rating"]) / 100
  return df

# Use these if you want to upload the file directly to the runtime
# reviews = read_file('asin_B0051VVOB2.json')
# reviews = read_file('asin_B0054JZC6E.json')

# This is a google drive path specific to my GDrive, you can also modify this if you want
reviews = read_file('drive/MyDrive/Colab Notebooks/CS 184 Final Project/results.txt')

"""# EDA"""

reviews

np.mean(reviews["overall"])

print("Total reviews:", len(reviews["helpful_ratio"]))

# Count number of 0/0
print("Num 0/0 reviews:", sum(reviews["helpful_ratio"].isna()))

# Count number of non-nas
print("Num useful reviews:", len(reviews["helpful_ratio"]) - sum(reviews["helpful_ratio"].isna()))

reviews["helpful_ratio"].describe()

# Histograph of helpfulness
plt.hist(reviews["helpful_ratio"])
plt.title("$h_n$ Across Reviews for B0051VVOB2")
plt.xlabel("$h_n$")
plt.ylabel("Frequency")

plt.show()

# Histograph of helpfulness per each review rating
fig, ax = plt.subplots(1, 1)
ratings = [1.0, 2.0, 3.0, 4.0, 5.0]
density_mat = np.empty((len(ratings), 10))
for i, rating in enumerate(ratings):
  df = reviews[reviews['overall'] == rating]
  density, _ = np.histogram(df["helpful_ratio"], range=(0, 1), density=True)
  density_mat[i, :] = density * 0.1

mat = ax.matshow(density_mat)
for (i, j), val in np.ndenumerate(density_mat):
    ax.text(j, i, '{:0.2f}'.format(val), ha='center', va='center')

ax.xaxis.set_ticks_position('bottom')
ax.set_xticks(np.arange(11) - 0.5)
ax.set_xticklabels(['{:0.1f}'.format(i * 0.1) for i in range(11)])
ax.set_xlabel("$h_n$")
ax.set_ylabel("Review Rating")
ax.set_yticklabels([''] + ratings)
ax.set_title("$h_n$ Across Different Review Ratings for B0051VVOB2")
fig.show()

plt.scatter(reviews["helpful_ratio"], reviews["llm_rating"])
plt.xlabel("Actual helpfulness rating")
plt.ylabel("LLM-estimated rating")
plt.title("LLM-estimated ratings vs. actual helpfulness ratings")

"""### MAB Implementation"""

# Helper functions
seed = 184
rng = np.random.default_rng(seed)

def random_argmax(a):
    '''
    Select the index corresponding to the maximum in the input list.
    Ties are randomly broken.
    '''
    return rng.choice(np.where(a == a.max())[0])

# Based on the implementation from HW 2
class MAB:
    def __init__(self, mu_list, T=100, init_record=None):
        '''
        Parameters:
            T: horizon
            mu_list: list of true values for bandits
            seed: random seed to make testing consistent
        '''
        self.__K = len(mu_list)
        self.__mu_list = mu_list
        self.__T = T
        # Regret incurred by each arm
        self.__arm_regrets = np.max(mu_list) - mu_list
        self.__init_record = init_record

        # Current time
        self.__t = 0
        # [Num rewards of 0, Num rewards of 1] for each arm
        self.__record = np.zeros((self.__K,2)) if init_record is None else np.copy(init_record)
        # History of regret
        self.__regrets = np.empty(T)

        # Precalculate random numbers to speed things up
        self.__random = rng.random(T)

    def pull(self, ind):
        '''
        Pull the bandit arm with index ind
        '''
        reward = 1 * (self.__random[self.__t] < self.__mu_list[ind])
        self.__record[ind, reward] += 1
        self.__regrets[self.__t] = self.__arm_regrets[ind]
        self.__t += 1
        return reward

    def reset(self):
        self.__t = 0
        self.__record = np.zeros((self.__K,2)) if self.__init_record is None else np.copy(self.__init_record)
        self.__regrets = np.empty(self.__T)

        # Precalculate random numbers to speed things up
        self.__random = rng.random(self.__T)

    def get_record(self):
        return self.__record

    def get_regrets(self):
        return np.cumsum(self.__regrets)

    def get_T(self):
        return self.__T

    def get_K(self):
        return self.__K

class UCB:
    def __init__(self, MAB, delta=0.05):
        self.MAB = MAB
        self.delta = delta

        self.ucb = np.empty(self.MAB.get_K())
        self.init_ucb()

    def init_ucb(self):
      for arm in range(self.MAB.get_K()):
        wins = self.MAB.get_record()[arm, 1]
        n = wins + self.MAB.get_record()[arm, 0]

        if n == 0:
          self.ucb[arm] = np.inf
        else:
          ci = np.sqrt(np.log(2 * self.MAB.get_K() * self.MAB.get_T() / self.delta) / (2 * n))
          self.ucb[arm] = wins/n + ci

    def reset(self):
        '''
        Reset the instance and eliminate history.
        '''
        self.MAB.reset()
        self.ucb = np.empty(self.MAB.get_K())
        self.init_ucb()

    def play_one_step(self):
        arm = random_argmax(self.ucb)
        self.MAB.pull(arm)

        # Update CI of arm
        wins = self.MAB.get_record()[arm, 1]
        n = wins + self.MAB.get_record()[arm, 0]
        ci = np.sqrt(np.log(2 * self.MAB.get_K() * self.MAB.get_T() / self.delta) / (2 * n))
        self.ucb[arm] = wins/n + ci

class Thompson_sampling:
    def __init__(self, MAB):
        self.MAB = MAB

    def reset(self):
        '''
        Reset the instance and eliminate history.
        '''
        self.MAB.reset()

    def play_one_step(self):
        '''
        Implement one step of the Thompson sampling algorithm.
        '''
        thetas = rng.beta(self.MAB.get_record()[:, 1] + 1, self.MAB.get_record()[:, 0] + 1)
        arm = random_argmax(thetas)
        reward = self.MAB.pull(arm)

"""### Experiments"""

# For the sake of our experiment, we only want reviews that have a rating
good_reviews = reviews[reviews["helpful_ratio"].notna()]
good_reviews

def run_mab(algo, n=30):
  """
    Runs the MAB algorithm for a given number of trials. Calculates average regret as well as 5% and 95% CIs around regret.

    Params:
      algo: The MAB algorithm to run (UCB or Thompson_sampling)
      n: Number of trials
  """
  T = algo.MAB.get_T()
  run_regrets = np.empty((n, T))
  for i in trange(n, desc=f"{type(algo).__name__} runs"):
    algo.reset()
    for _ in range(T):
      algo.play_one_step()
    run_regrets[i,:] = algo.MAB.get_regrets()

  regret_avg = np.mean(run_regrets, axis=0)
  regret_5 = np.percentile(run_regrets, 5, axis=0)
  regret_95 = np.percentile(run_regrets, 95, axis=0)

  return (regret_avg, regret_5, regret_95)

# Parameters
hn = good_reviews["helpful_ratio"].to_numpy()
T = 10000

mab = MAB(hn, T=T)
ucb = UCB(mab)
thompson = Thompson_sampling(mab)
u_avg, u_5, u_95 = run_mab(ucb)
t_avg, t_5, t_95 = run_mab(thompson)

def get_init_record(M):
  """
    We initialize the record by pretending we had M people rate the review beforehand,
    based on the given llm_rating.
  """
  init_record = np.empty((len(good_reviews), 2))
  init_record[:, 0] = (1 - good_reviews["llm_rating"]) * M
  init_record[:, 1] = good_reviews["llm_rating"] * M

  return init_record

init_record = get_init_record(10)
llm_mab = MAB(hn, init_record=init_record, T=T)

llm_ucb = UCB(llm_mab)
llm_thompson = Thompson_sampling(llm_mab)
l_u_avg, l_u_5, l_u_95 = run_mab(llm_ucb)
l_t_avg, l_t_5, l_t_95 = run_mab(llm_thompson)

fig, ax = plt.subplots(1, 1, figsize=(10, 5))

steps = np.arange(T)
ax.plot(steps, u_avg, 'b-', label="UCB (Standard)")
ax.fill_between(steps, u_5, u_95, color='b', alpha=.1)
ax.plot(steps, t_avg, 'r-', label="Thompson (Standard)")
ax.fill_between(steps, t_5, t_95, color='r', alpha=.1)

ax.plot(steps, l_u_avg, 'b--', label="UCB (LLM)")
ax.fill_between(steps, l_u_5, l_u_95, color='b', alpha=.1)
ax.plot(steps, l_t_avg, 'r--', label="Thompson (LLM)")
ax.fill_between(steps, l_t_5, l_t_95, color='r', alpha=.1)

ax.set_xlabel('Step')
ax.set_ylabel('Cumulative Regret')
ax.set_title('Cumulative Regret for Different Algorithms')
ax.set_ylim
ax.legend()

plt.show()

print(u_avg[-1], u_5[-1], u_95[-1])
print(t_avg[-1], t_5[-1], t_95[-1])
print(l_u_avg[-1], l_u_5[-1], l_u_95[-1])
print(l_t_avg[-1], l_t_5[-1], l_t_95[-1])

# Some basic exploration of how the hyperparameter M impacts regret
Ms = [5, 10, 20, 50, 100]
cs = ['r', 'g', 'y', 'c', 'm']

fig, ax = plt.subplots(1, 1, figsize=(10, 5))

ax.plot(steps, t_avg, 'r-', label="Thompson (Standard)")
ax.fill_between(steps, t_5, t_95, color='b', alpha=.1)

for i in range(len(Ms)):
  init_record = get_init_record(Ms[i])
  llm_mab = MAB(hn, init_record=init_record, T=T)

  llm_thompson = Thompson_sampling(llm_mab)
  l_t_avg, l_t_5, l_t_95 = run_mab(llm_thompson)

  ax.plot(steps, l_t_avg, f'{cs[i]}--', label=f"Thompson (LLM, $M={Ms[i]}$)")
  ax.fill_between(steps, l_t_5, l_t_95, color=cs[i], alpha=.1)


ax.set_xlabel('Step')
ax.set_ylabel('Cumulative Regret')
ax.set_title('Cumulative Regret for Different $M$')
ax.legend()